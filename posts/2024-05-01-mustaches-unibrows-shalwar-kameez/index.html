<!doctype html><html xmlns=http://www.w3.org/1999/xhtml xml:lang=en-us lang=en-us><head><link href=https://gmpg.org/xfn/11 rel=profile><meta charset=utf-8><meta name=generator content="Hugo 0.150.1"><meta name=viewport content="width=device-width,initial-scale=1"><title>Mustaches, Unibrows, and Shalwar Khameezes &#183; Nabeel Siddiqui</title><meta name=description content><link type=text/css rel=stylesheet href=/css/print.min.46d9bcbd80937d55db06b7034dfed7fc48b2bfb13e535dd9eac5581bfbaa8ddf.css media=print integrity="sha256-Rtm8vYCTfVXbBrcDTf7X/Eiyv7E+U13Z6sVYG/uqjd8=" crossorigin=anonymous><link type=text/css rel=stylesheet href=/css/hyde-bundle.min.8a8e68cd8a30f59721938f04a39b6e8ef3863714d4c0a7e902cd7c63ef9e2250.css media=all integrity="sha256-io5ozYow9Zchk48Eo5tujvOGNxTUwKfpAs18Y++eIlA=" crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700"><link rel=apple-touch-icon-precomposed sizes=144x144 href=/apple-touch-icon-144-precomposed.png><link rel="shortcut icon" href=/favicon.png></head><body><aside class=sidebar><div class="container sidebar-sticky"><div class=sidebar-about><a href=https://nabeelsiddiqui.net/><h1>Nabeel <span class=last-name>Siddiqui</span></h1></a></div><nav><ul class=sidebar-nav><li><a href=/>Home</a></li><li><a href=/about/>About</a></li><li><a href=/research/>Research</a></li><li><a href=/teaching/>Teaching</a></li><li><a href=/cv/>CV</a></li></ul></nav><p>&copy; 2025. All rights reserved.</p></div></aside><main class="content container"><div class=post><h1>Mustaches, Unibrows, and Shalwar Khameezes</h1><time datetime=2024-05-01T00:00:00Z class=post-date>Wed, May 1, 2024</time><p>In April 2024, <em>The Verge</em> posted a story about attempting to <a href=https://www.theverge.com/2024/4/10/24122072/ai-generated-asian-man-white-woman-couple-gemini-dalle-midjourney-tests>create an Asian man with a white woman</a>. The article found that many image generators gave the woman &ldquo;Asian features.&rdquo; Because the majority of these image generators are trained on datasets that predominantly feature white individuals, the AI struggled to accurately represent an Asian man without relying on stereotypes and tropes.</p><p>At one point, <a href=https://www.theverge.com/2024/4/4/24121419/meta-instagram-ai-image-generator-asian-race><em>Meta</em> banned keywords</a> that were related to Asians. Likewise, <em>Google</em> paused <a href=https://www.theverge.com/2024/2/22/24079876/google-gemini-ai-photos-people-pause><em>Gemini&rsquo;s</em> ability to generate images of people</a> after concerns about diversity. There are some attempts at fixing this bias through the use of &ldquo;Diversity Fine-tuning&rdquo;:</p><div style=display:flex;justify-content:center><iframe src="https://www.youtube.com/embed/5zLxoiBdG2U?si=wS4i08O6TJ-n2OJ_" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy=strict-origin-when-cross-origin allowfullscreen style=aspect-ratio:16/9;width:100%!important></iframe></div><p>However, this requires that there be a significant amount of photos of the group in question. Growing up as a Pakistani in Mississippi, I was skeptical that a large dataset of photos representing people like me would exist. Out of curiosity, I decided to try generating images using various AI image generators with prompts related to my background. The results were&mldr; interesting, to say the least. <a href=https://en.wikipedia.org/wiki/Mississippi#Demographics>Despite the fact that African Americans make up almost thirty-eight percent of Mississippians</a>, Mississippi&rsquo;s cultural image is predominantly white. Often, Southern states are conflated with Texas, leading to depictions with cowboy boots and hats, even though these weren&rsquo;t common in my experience.</p><p>I decided to use a simple prompt of &ldquo;Pakistani boy from Mississippi&rdquo; across nine different AI image generators. While some produced plausible pictures of Pakistani boys—one even in a polo shirt—the majority relied on stereotypes. Most images placed the boys in Shalwar Kameez, attire I rarely saw Pakistani children wear in Mississippi outside specific cultural events. Even stranger were stereotypes I hadn&rsquo;t encountered before, largely oriented around facial hair. The generated boys often had unibrows, and some even sported full mustaches—an unrealistic depiction for children. The idea of &ldquo;hairy&rdquo; Pakistani children wasn&rsquo;t a prominent trope I recall hearing growing up, making it interesting to see how the AI internalized and applied these particular stereotypes.</p><p>Furthermore, the generated images generally lacked regional specificity. Despite the prompt including &ldquo;from Mississippi,&rdquo; the AI struggled to incorporate visual cues representing that cultural or geographical context, such as clothing or surroundings typical of the American South. Instead, the generators defaulted heavily to stereotypical markers of Pakistani identity, ignoring the regional modifier.</p><p>Below, you can observe the results that I got for the various image generators:</p><div class="flourish-embed flourish-cards" data-src=visualisation/18195939><script src=https://public.flourish.studio/resources/embed.js></script></div></div></main></body></html>