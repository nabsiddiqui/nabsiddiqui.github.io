---
title: The GLM as a Means to Ensure Statistical Validity
layout: post
author: "Nabeel Siddiqui"
---

This was published a little while ago, but I haven't had a chance to share it until: my latest article in the International Journal of Digital Humanities1! It tackles a challenge I've seen many of us in the Digital Humanities (DH) dealing with and that I've been thinking about for a while. Namely, how do we ensure our quantitative research stands up to scrutiny? In 2019, Nan Z. Da published an article in *Critical Inquiry* entitled ["The Computational Case against Computational Literary Studies."](https://www.journals.uchicago.edu/doi/abs/10.1086/702594?journalCode=ci&journalCode=ci) The response was a fierce debate about the validity of quantitative methods in the humanities. In response, *Critical Inquiry* hosted an [online forum](https://critinq.wordpress.com/2019/03/31/computational-literary-studies-a-critical-inquiry-online-forum/) to discuss the issues raised by Da's article. The responses focused heavily on some of the errors that Da made in her analysis, but I think the real issue is that many of us in DH lack the statistical training to conduct quantitative research effectively.

In response, my article focuses on the : the General Linear Model (GLM) for better statistical training and what I call a "minimal research compendium"